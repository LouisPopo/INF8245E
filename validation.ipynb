{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# The non-tuned parameters dict used to initialize the model.\n",
    "#   params ={'random_state': 2,\n",
    "#            'class_weight': \"balanced\"}\n",
    "\n",
    "# When isExhaustive is True, the dictionary(ies) must associate lists of values to each hyperparamter. \n",
    "# These the hyperparameter value combinations are exhaustively tested in the cross validation process.\n",
    "#   exhaustive_hyperparams = {'max_leaf_nodes': list(range(45, 60)),\n",
    "#          'max_depth': [112, 114, 118, 119] }\n",
    "\n",
    "# When isExhaustive is False, the dictionary(ies) must associate scipy.stats distributions or list of values to each hyperparamter. \n",
    "# These distributions/lists are sampled in the cross validation process.\n",
    "#   hyperparams = {'max_leaf_nodes': randint(10,100),\n",
    "#            'max_depth': randint(10, 100) }\n",
    "\n",
    "def getValidHyperparams(X_train, y_train, modelConstructor, params, hyperparams, isRandom=True):\n",
    "  model =  modelConstructor(**params)\n",
    "  # cv can take a CV Splitter object... make our own custom for more control than the automatic stratified splitting? see https://scikit-learn.org/stable/glossary.html#term-CV-splitter\n",
    "  # cv=5 in itself is a hyper parameter that we should tune... manually? Values 3 and 4 change very little performance wise.\n",
    "  model_cv = RandomizedSearchCV(model, hyperparams, cv=4, scoring='f1_micro', n_jobs=1, return_train_score=True) if isRandom else GridSearchCV(model, hyperparams, cv=4, scoring='f1_micro', n_jobs=1, return_train_score=True)\n",
    "\n",
    "  cv_results = model_cv.fit(X_train, y_train)\n",
    "\n",
    "  print(cv_results.best_params_)\n",
    "  return cv_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of how it can be used. Copy this cell and the one above i the main ipynb to test. This can run for quite a while (20min +)\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "params = {'random_state': 2,\n",
    "         'class_weight': \"balanced\"}\n",
    "hyperparams = {'max_leaf_nodes': randint(10,100),\n",
    "              'max_depth': randint(10, 100),\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "\n",
    "val_hyperparams = getValidHyperparams(X_train, y_train, RandomForestClassifier, params, hyperparameters=hyperparams)\n",
    "RF = RandomForestClassifier(**val_hyperparams, **params)\n",
    "RF.fit(X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
